{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import asyncio\n",
    "from pinecone import Pinecone\n",
    "from google.cloud import speech\n",
    "from google.cloud import vision\n",
    "from langgraph.graph import Graph\n",
    "from typing import Dict, Any\n",
    "from core.config import PINECONE_API_KEY, GOOGLE_CREDENTIALS_PATH\n",
    "\n",
    "class ChatAgent:\n",
    "    def __init__(self):\n",
    "        # Set up logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "        self.logger.addHandler(handler)\n",
    "\n",
    "        # Initialize Pinecone\n",
    "        self.pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        self.index = self.pc.Index(\"troubleshooter-index\")\n",
    "\n",
    "        # Initialize Google Cloud clients\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GOOGLE_CREDENTIALS_PATH\n",
    "        self.speech_client = speech.SpeechClient()\n",
    "        self.vision_client = vision.ImageAnnotatorClient()\n",
    "\n",
    "        # Set up LangGraph\n",
    "        self.graph = Graph()\n",
    "        self.graph.add_node(\"input\", self.input_node)\n",
    "        self.graph.add_node(\"text_processing\", self.text_processing_node)\n",
    "        self.graph.add_node(\"voice_processing\", self.voice_processing_node)\n",
    "        self.graph.add_node(\"video_processing\", self.video_processing_node)\n",
    "        self.graph.add_node(\"issue_identification\", self.issue_identification_node)\n",
    "        self.graph.add_node(\"solution_retrieval\", self.solution_retrieval_node)\n",
    "        self.graph.add_node(\"response_generation\", self.response_generation_node)\n",
    "\n",
    "        self.graph.add_conditional_edges(\n",
    "            \"input\",\n",
    "            lambda state: state[\"input_type\"],\n",
    "            {\n",
    "                \"text\": \"text_processing\",\n",
    "                \"voice\": \"voice_processing\",\n",
    "                \"video\": \"video_processing\"\n",
    "            }\n",
    "        )\n",
    "        self.graph.add_edge(\"text_processing\", \"issue_identification\")\n",
    "        self.graph.add_edge(\"voice_processing\", \"issue_identification\")\n",
    "        self.graph.add_edge(\"video_processing\", \"issue_identification\")\n",
    "        self.graph.add_edge(\"issue_identification\", \"solution_retrieval\")\n",
    "        self.graph.add_edge(\"solution_retrieval\", \"response_generation\")\n",
    "        self.graph.set_entry_point(\"input\")\n",
    "        self.graph.set_finish_point(\"response_generation\")\n",
    "        self.app = self.graph.compile()\n",
    "\n",
    "    def input_node(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        self.logger.info(f\"Received input: type={state.get('input_type')}, data={state.get('input_data')}\")\n",
    "        return state\n",
    "\n",
    "    async def text_processing_node(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        state[\"processed_input\"] = state.get(\"input_data\", \"\")\n",
    "        self.logger.info(f\"Processed text input: {state['processed_input']}\")\n",
    "        return state\n",
    "\n",
    "    async def voice_processing_node(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        audio_file = state.get(\"input_data\", \"\")\n",
    "        if audio_file and os.path.exists(audio_file):\n",
    "            try:\n",
    "                with open(audio_file, 'rb') as f:\n",
    "                    content = f.read()\n",
    "                audio = speech.RecognitionAudio(content=content)\n",
    "                config = speech.RecognitionConfig(\n",
    "                    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "                    sample_rate_hertz=16000,\n",
    "                    language_code='en-US'\n",
    "                )\n",
    "                response = self.speech_client.recognize(config=config, audio=audio)\n",
    "                for result in response.results:\n",
    "                    state[\"processed_input\"] = result.alternatives[0].transcript\n",
    "                    self.logger.info(f\"Processed voice input: {state['processed_input']}\")\n",
    "                    return state\n",
    "                state[\"processed_input\"] = \"No speech detected\"\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing voice: {e}\")\n",
    "                state[\"processed_input\"] = \"Error processing voice\"\n",
    "        else:\n",
    "            state[\"processed_input\"] = \"No audio file provided\"\n",
    "        return state\n",
    "\n",
    "    async def video_processing_node(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        video_file = state.get(\"input_data\", \"\")\n",
    "        if video_file and os.path.exists(video_file):\n",
    "            try:\n",
    "                # Placeholder: Extract a frame or analyze video\n",
    "                with open(video_file, 'rb') as f:\n",
    "                    content = f.read()\n",
    "                image = vision.Image(content=content)  # Note: Needs video frame extraction\n",
    "                response = self.vision_client.label_detection(image=image)\n",
    "                labels = response.label_annotations\n",
    "                state[\"processed_input\"] = \", \".join([label.description for label in labels])\n",
    "                self.logger.info(f\"Processed video input: {state['processed_input']}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing video: {e}\")\n",
    "                state[\"processed_input\"] = \"Error processing video\"\n",
    "        else:\n",
    "            state[\"processed_input\"] = \"No video file provided\"\n",
    "        return state\n",
    "\n",
    "    async def issue_identification_node(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        processed_input = state.get(\"processed_input\", \"\").lower()\n",
    "        self.logger.info(f\"Identifying issue from: {processed_input}\")\n",
    "        if \"no sound\" in processed_input:\n",
    "            state[\"issue\"] = \"no_sound\"\n",
    "        elif \"tv not turning on\" in processed_input:\n",
    "            state[\"issue\"] = \"tv_not_turning_on\"\n",
    "        elif \"settings\" in processed_input:\n",
    "            state[\"issue\"] = \"settings_issue\"\n",
    "        elif \"flashing light\" in processed_input or \"error code\" in processed_input:\n",
    "            state[\"issue\"] = \"error_code\"\n",
    "        else:\n",
    "            state[\"issue\"] = \"unknown\"\n",
    "        self.logger.info(f\"Identified issue: {state['issue']}\")\n",
    "        return state\n",
    "\n",
    "    async def solution_retrieval_node(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        # Try Pinecone first\n",
    "        try:\n",
    "            query_vector = [0.1] * 1536  # Placeholder; replace with actual embedding\n",
    "            results = self.index.query(vector=query_vector, top_k=1, include_metadata=True)\n",
    "            if results['matches']:\n",
    "                state[\"solution\"] = results['matches'][0]['metadata'].get('solution', \"No solution found\")\n",
    "                self.logger.info(f\"Retrieved solution from Pinecone: {state['solution']}\")\n",
    "                return state\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pinecone query failed: {e}\")\n",
    "\n",
    "        # Fallback to hardcoded solutions\n",
    "        solutions = {\n",
    "            \"no_sound\": \"Please check if the sound system is turned on and cables are connected.\",\n",
    "            \"tv_not_turning_on\": \"Please check the power cable and ensure the TV is plugged in.\",\n",
    "            \"settings_issue\": \"Navigate to the settings menu and verify the correct input source is selected.\",\n",
    "            \"error_code\": \"The flashing light indicates an error; please note the pattern and consult the device manual.\"\n",
    "        }\n",
    "        state[\"solution\"] = solutions.get(state[\"issue\"], \"Issue not recognized. Please provide more details.\")\n",
    "        self.logger.info(f\"Retrieved fallback solution: {state['solution']}\")\n",
    "        return state\n",
    "\n",
    "    async def response_generation_node(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        state[\"response\"] = state.get(\"solution\", \"No solution found.\")\n",
    "        self.logger.info(f\"Generated response: {state['response']}\")\n",
    "        return state\n",
    "\n",
    "    async def process_input(self, input_type: str, input_data: str) -> str:\n",
    "        \"\"\"Main method to process user input and return a response.\"\"\"\n",
    "        state = {\"input_type\": input_type, \"input_data\": input_data}\n",
    "        result = await self.app.invoke(state)\n",
    "        return result[\"response\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the agent locally\n",
    "    async def test():\n",
    "        agent = ChatAgent()\n",
    "        # Test text input\n",
    "        response = await agent.process_input(\"text\", \"My TV has no sound.\")\n",
    "        print(f\"Text Response: {response}\")\n",
    "        # Test voice input (requires a real audio file)\n",
    "        response = await agent.process_input(\"voice\", \"/home/vincent/ixome/test_audio.wav\")\n",
    "        print(f\"Voice Response: {response}\")\n",
    "        # Test video input (requires a real video file)\n",
    "        response = await agent.process_input(\"video\", \"/home/vincent/ixome/test_video.mp4\")\n",
    "        print(f\"Video Response: {response}\")\n",
    "\n",
    "    asyncio.run(test())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ixome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
